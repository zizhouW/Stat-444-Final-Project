---
title: "444 project code"
author: "Zizhou Wang"
date: "April 6, 2018"
output: pdf_document
---

# Motivation and introduction of the problem

After seeing our final dataset, we raised the question of if it is possible to predict an NBA player's salary based on his previous year's performance. Our project consists of three major parts, smoothing spline, random foresets, and boosting. We used smoothing spline to model our data, by testing different models constructed by different combinations of explanatory variates, we were able to get the best model for our data on hand to predict players' salary. We used random forest to find the importance of our explanatory variates, and we were able to find the most important variates to minimize the error. We were also able to find the importance of each variable using the gradient boosting method.

# Data

```{r}
data <- read.csv("./combined.csv", header=TRUE)
data$WR = data$W/data$GP
head(data)
```


One of the most straight forward way to evaluate the performance of an NBA player is to look at his "Points per Game", "Assists per Game", and "Rebounds per Game", which are the 3 most mentioned statistics when NBA analysts and fans make comparison to players. We initially tried to find a relationship between the PRA(Points + Rebounds + Assists per game) and the Salary of an NBA player. However, we realized that it will almost always introduce a bias, because it does not tell us the full image of the player's ability. For example, Points are usually easier to get compared to Assists and Rebounds. When a player scores, they will either get two points or three points, potentially earning an extra Free Throw, which counts as one more point. When a player gets an Assist or a Rebound, the count only goes up by 1. Having 10 Rebounds or 10 Assists after a game is considered a good performance, but having 10 Points for a game is usually average. The PRA also introduces a heavier weight on the player's offensive ability than his defensive ability on the court, since Points, Assists, and Offensive Rebounds all happen at the front court. Therefore, we found a better way to determine the efficiency of an NBA player, which is to look at his EFF, calculated by EFF = PTS + REB + AST + STL + BLK - FGM - FTM - TOV, where all variates are averaged per game. The EFF takes Steal (STL), Block (BLK), Field Goal Missed (FGM), Free Throw Missed (FTM), and Turn Over (TOV) into account, which adds the defensive ability (STL and BLK) and inefficiency (FGM, FTM, TO) into the equation.

```{r echo=FALSE}
data$PRA = data$PTS + data$REB + data$AST + data$STL + data$BLK - data$TOV - data$FGM - data$FTM
x = data$PRA
y = log(data$Salary)
breaks_v <- seq(min(x), max(x), length.out=15)
nbhd_v <- cut(x,
  breaks= breaks_v,
  include.lowest=TRUE
)
# varying size, constant proportions
breaks_p <- c(quantile(x, seq(0,1,0.1) ))
nbhd_p <- cut(x,
  breaks=breaks_p,
  include.lowest=TRUE
)

local_v <- levels(nbhd_v)
local_p <- levels(nbhd_p)
# Compute the local averages
get_ave <- function(locals, nbhds) {
mu <- vector(mode="numeric", length=length(x))
for (i in 1:length(locals)) {
nbhd_i <- nbhds == locals[i]
mu[nbhd_i] <- mean(y[nbhd_i])
}
mu
}
mu_v <- get_ave(local_v, nbhd_v)
mu_p <- get_ave(local_p, nbhd_p)
# A quick and dirty way to draw the mu by neighbourhood
plot_ave <- function(locals,
nbhds,
x,
mu,
...)
{for (i in 1:length(locals)) {
nbhd_i <- nbhds == locals[i]
newx <- x[nbhd_i]
newmu <- mu[nbhd_i]
Xorder <- order(newx)
if(length(newx)==1){
lines(rep(newx[Xorder],2),
rep(newmu[Xorder],2),
...)
} else {
lines(newx[Xorder], newmu[Xorder], ...)
}
}
}
# plot
plot(x,y,xlab="EFF", ylab="log(Salary)",
col="grey80", pch=19, cex=0.5,
main = "Constant width nbhd")
plot(x,y,xlab="EFF", ylab="log(Salary)",
col="grey80", pch=19, cex=0.5,
main = "Constant width nbhd")
plot_ave(local_v, nbhd_v, x, mu_v,
col="blue", lwd=5)
plot(x,y,xlab="EFF", ylab="log(Salary)",
col="grey80", pch=19, cex=0.5,
main = "Constant proportion nbhd")
plot_ave(local_p, nbhd_p, x, mu_p,
col="red", lwd=5)
```
We first want to see what our data look like when EFF is plotted aginst log(Salary), even though our data look to be bimodel, we can still observe an increasing trend, according to the piece wise fitting using neighbourhood.

\newpage

# Data Preprocessing

We initially started looking at the data for salary of NBA players at https://www.basketball-reference.com/contracts/players.html (updates constantly), which had 582 records of player salaries for year 2017-2018 at the time. However, we had to remove some duplicated records for players with different salaries on different teams. This is because some players could get cut by teams half way through the season, and sometimes they would get picked up by another team, which resulted in having multiple player contracts in a year. An example for this is Rajon Rondo, who was waived by the Chicago Bulls, signed a contract with New Orleans Pelicans right after.

During the process of matching players’ statistics with their salaries, we encountered some cases where some the player information for a couple of players listed in our salary could not be found. An example is Walt Lemon, Jr., who is initially listed in our salary data. We were not able to find his player information on https://stats.nba.com/players/bio/, which contains data that we thought could be important in our analysis. Therefore, we removed these records.

For the “Position” categorical variable in our dataset, we stated in our proposal that we would be using 5 values, PG (Point Guard), SG (Shooting Guard), SF (Small Forward), PF (Power Forward), and C (Center). It turns out that many guards in the NBA today are “combo guards”, which means they can both play at the Point Guard and Shooting Guard position (e.g. James Harden). There are also many forwards in the NBA who can both play at the Small Forward and Power Forward position (e.g. Lebron James). We reduced the number of values to 3, grouping PG and SG as G (Guard), SF and PF as F (Forward). In addition, there are some players who are “swingman”, meaning they can both play at the SG and SF position (e.g. Jimmy Butler). Since this is not a frequent case, we chose a position for each of them based on which position they had mostly been playing at this season (2017-2018) and our knowledge to the players.

Our eventually obtained our final dataset, which contains 515 records and does not contain any N/A’s.

We then realized a couple outliers in our dataset. For example, Gordon Hayward was horribly injured during his very first game at the beginning of the year. He was not able to return for the rest of the season. With the 4th highest salary on our list, he would be an extreme outlier in our models with minimal statistical contribution. However, this does not mean that he is not worth the salary, since he was only able to play for about 5 minutes before the injury. Therefore, we would like to exclude him when building our models, along with several other players in similar conditions.

\newpage

# Smoothing

```{r echo=FALSE}
## Smoothing, all
library(splines)
library(MASS)
p <- 3
x = data$PRA
y = log(data$Salary)

knots_p <- quantile(x, seq(0.2, 0.9, 0.1))
Xmat <- bs(x, degree= p, knots=knots_p)
Xorder <- order(x)
blim <- extendrange(Xmat)
parOptions <- par(mfrow = c(2,2))
for (j in 1:ncol(Xmat)) {
  plot(x[Xorder], Xmat[Xorder,j],
  type="l",
  ylim=blim,
  xlim = extendrange(x),
  xlab="EFF", ylab="Basis",
  main=paste("Basis vector", j),
  col="steelblue")
}
par(parOptions)
```
We then try to fit a cubic spline to our data. First we need to get its basis functions for our fitted model, which can be illustrated by plotting them as a function of EFF.
The basis functions are clearly not polynomials. The estimated smooth will be a linear combination of these basis functions.

```{r echo=FALSE}
## cubic spline
fit <- lm(y ~ bs(x, degree= p, knots=knots_p))
xrange <- extendrange(x)
xnew <- seq(min(xrange), max(xrange), length.out=515)
ypred <- predict(fit,newdata=data.frame(x=xnew))
plot(x,y,xlab="EFF", ylab="Salary",
  col="grey80", pch=19, cex=0.5,
  main = "Cubic Spline")
lines(xnew, ypred, col="darkgreen", lwd=2, lty=1)
summary(fit)
```
We then fitted the cubic spline to the data.
```{r echo=FALSE}
## bisqure
fit2 <- rlm(y ~ bs(x, degree= p, knots=knots_p), psi=psi.bisquare)
ypred2 <- predict(fit2,newdata=data.frame(x=xnew))
plot(x,y,xlab="EFF", ylab="Salary",
  col="grey80", pch=19, cex=0.5,
  main = "Bisquare fit cubic spline")
lines(xnew, ypred2, col="firebrick", lwd=2, lty=1)

```

```{r echo=FALSE}
## smoothing spling
df <- 11
sm <- smooth.spline(x, y, df = df)
ypred.sm <- predict(sm, x=xnew)$y
plot(x,y,xlab="EFF", ylab="Salary",
col="grey80", pch=19, cex=0.5,
main = paste("Smoothing spline, df =", df)
)
lines(xnew, ypred.sm, col="steelblue", lwd=2)
```


```{r echo=FALSE}
## Cross Validation
getmubar<-function(muhats) {
  function(x){
    Ans <-sapply(muhats, FUN=function(muhat){muhat(x)})
    apply(Ans, MARGIN=1, FUN=mean)
  }
}
ave_y_mu_sq <- function(sample, predfun){
  mean(abs(sample$y - predfun(sample$x))^2)
}
ave_mu_mu_sq <- function(predfun1, predfun2,x){
  mean((predfun1(x)- predfun2(x))^2)
}

apse <- function(Ssamples, Tsamlpes, fitFunc, df){
  N_S <-length(Ssamples)
  mean(sapply(1:N_S,
       FUN = function(j){
         S_j <- Ssamples[[j]]
         muhat <- fitFunc(S_j, df=df)
         T_j <- Tsamlpes[[j]]
         ave_y_mu_sq(T_j,muhat)
       })
  )
}

kfold <-function(N,k=N,indices=NULL){
  if (is.null(indices)) {
    indices <-sample(1:N, N,replace=FALSE)
    } else {
      N <-length(indices)
    }
  
  if (k >N)stop("k must not exceed N")
  gsize <-rep(round(N/k), k)
  extra <-N -sum(gsize)
  if (extra >0) {
    for (i in 1:extra) {
      gsize[i] <-gsize[i] +1
    }
  }
  if (extra <0) {
    for (i in 1:abs(extra)) {
      gsize[i] <-gsize[i] -1
    }
  }
  running_total <-c(0,cumsum(gsize))
  lapply(1:k,
         FUN=function(i) {
           indices[seq(from =1+running_total[i],
                       to =running_total[i+1],
                       by =1)]
         }
  )
}
getKfoldSamples <-function (x, y, k,indices=NULL){
  groups <-kfold(length(x), k, indices)
  Ssamples <-lapply(groups,
                    FUN=function(group) {
                      list(x=x[-group],y=y[-group])
                      })
  Tsamples <-lapply(groups,
                    FUN=function(group) {
                      list(x=x[group],y=y[group])
                      })
  list(Ssamples =Ssamples,Tsamples =Tsamples)
}

#named with testwithv because when you fold k you get v!
testwithv <- function(x,y,k, fitFunc, df) {
  samples = getKfoldSamples(x,y,k)
  apse(samples$Ssamples, samples$Tsamples, fitFunc , df)
  
}

k=5
df = 3

#replace fitFunc wtih your function
set.seed(54321)
testwithv(x,y,k,
          fitFunc = function(sample, df){
            x = sample$x
            y = sample$y
            fit <- lm(y ~ bs(x, degree= p, knots=knots_p))
            muhat <- function(newX) {
              predict(fit, newdata = list(x=newX))
            }
            muhat
          }, df)


```

\newpage

# Random Forest

We would like to utilize random forest to determine the importance of explanatory variates.
```{r echo=FALSE}
## Random Forest
library(randomForest)
get.explanatory_varnames <- function(formula){
  f <- as.formula(formula)
  terms <- terms(f)
  attr(terms, "term.labels")
}
set.seed(54321)
N <- nrow(data)
N_train <- round(2* N /3)
N_test <- N - N_train
id.train <- sample(1:N, N_train, replace=FALSE)
id.test <- setdiff(1:N, id.train)

##set.seed(1)
data.rf <- randomForest(log(Salary) ~
                          PTS + REB + AST + TOV + STL + BLK + Team + WR + AGE + FGM + 
                          FGPER + TPM + TPPER + FTM + FTPER + PF + PLUSMINUS + Position + Country + MIN + 
                          Draft.Round,
                        data = data,
                        importance = TRUE,subset = id.train,
                        mtry = 3)
importance(data.rf, type=2) #the drop in RSS
importance(data.rf, type=1) #average decrease in the accuracy of predictions
varImpPlot(data.rf)
trainy <- log(data[,"Salary"])
trainx <- data[, get.explanatory_varnames(data.rf)]
# Five fold cross-validataion
data.rfcv <- rfcv(trainx = trainx, trainy = trainy, cv.fold = 5)
# We can plot the results
with(data.rfcv, plot(n.var, error.cv, pch = 19, type="b", col="blue")) 
```
We used PTS, REB, AST, TOV, STL, BLK, Team, WR, AGE, FGM, FGPER, TPM, TPPER, FTM, FTPER, PF, PLUSMINUS, Position, Country, MIN, and Draft.Round against log(Salary) for the random forest. We did not choose to include Draft.Number because it is a categorical variate with 60 different potential values, but random forest does not accept categorical predictors with more than 53 categories.

The result suggests that the error of cross validation is the lowest for 10 explanatory variates, at about 1.18. We then choose the top 10 most important variates based on RSS, Team, MIN, FGM, Draft.Round, TOV, PF, PTS, AGE, REB, and FGPER, and run the process again.

```{r echo=FALSE}
set.seed(54321)
data.rfcv.10 <- randomForest(log(Salary) ~
                       Team + MIN + FGM + Draft.Round + TOV + PF + PTS + AGE + REB + FGPER,
                      data = data,
                      importance = TRUE)
trainx <- data[, get.explanatory_varnames(data.rfcv.10)]
# Five fold cross-validataion
data.rfcv <- rfcv(trainx = trainx, trainy = trainy, cv.fold = 5)
# We can plot the results
with(data.rfcv, plot(n.var, error.cv, pch = 19, type="b", col="blue"))
```
The result from the second run suggests that the error of cross validation is the lowest when there are 5 explanatory variates, at around 1.22. We then choose the top 5 most important variates againbased on RSS, which are Team, MIN, FGM, PTS, and AGE, and run the process again.

```{r echo=FALSE}
set.seed(54321)
data.rfcv.5 <- randomForest(log(Salary) ~
                       Team + MIN + FGM + Draft.Round + TOV,
                      data = data,
                      importance = TRUE)
trainx <- data[, get.explanatory_varnames(data.rfcv.5)]
data.rfcv <- rfcv(trainx = trainx, trainy = trainy, cv.fold = 5)
with(data.rfcv, plot(n.var, error.cv, pch = 19, type="b", col="blue"))
```
The result from the third run suggests that the error of cross validation is the lowest when there are 5 explanatory variates, at around 1.64. We can say that the Team, MIN, FGM, PTS, and AGE are important variates based on cross validation. Also, AGE seems to be the most important for predictive purposes.

## Player self-evaluate and improvements
For NBA players who would like to self-evaluate and who are trying to see what they can work on to receive a better contract, we can consider how Salary depends on just those explanatory variates that were under the control of the NBA player. Therefore, we removed Team, MIN, WR, AGE, Draft.Round, and PLUSMINUS. Age is obviously an uncontrollable variate. We think players rarely have control for Team, MIN, and Draft.Round since it does not depend on players' previous NBA performance, instead, these would depend on the decisions from coach and the organization Also, WR and PLUSMINUS have a lot to do with the teammates of the NBA player we are trying to analyze, so we decided to take these out of consideration as well. This move left us with 15 explanatory variates.
```{r echo=FALSE}
set.seed(54321)
data.rf2 <- randomForest(log(Salary) ~
                           PTS + REB + AST + TOV + STL + BLK + FGM + FGPER + TPM + 
                           TPPER + FTM + FTPER + PF + Position + Country,
                         data = data,
                         importance = TRUE,subset = id.train,
                         mtry = 3)
importance(data.rf2, type=2) #the drop in RSS
importance(data.rf2, type=1) #average decrease in the accuracy of predictions
varImpPlot(data.rf2)
trainy2 <- log(data[,"Salary"])
trainx2 <- data[, get.explanatory_varnames(data.rf2)]
data.rfcv2 <- rfcv(trainx = trainx2, trainy = trainy2, cv.fold = 5)
with(data.rfcv2, plot(n.var, error.cv, pch = 19, type="b", col="blue")) 
```
The cross validation suggests that all 15 explanatory variates are important, at an error of around 1.58. The result also suggests that FGM, TOV, REB, and PTS are the most important.

\newpage

# Boosting

We then used the Gradient Boosting method to determine the importance of explanatory variates, and see if it shows a different result compared to Random Forest.

```{r echo=FALSE}
library(gbm)
formula = "log(Salary)~ Team + AGE + PTS + MIN+REB + AST + TOV + STL +
            BLK + FGM + FGPER + TPM + TPPER + FTM + FTPER + PF + 
            Position + Country + PLUSMINUS + Draft.Round + WR"
explain_data = data[,c("Salary","PTS", "Team", "AGE","MIN", "PLUSMINUS", 
                       "Draft.Round", "REB", "AST", "TOV", "STL", "BLK", 
                       "FGM", "FGPER", "TPM", "TPPER", "FTM", "FTPER", 
                       "PF", "Position", "Country", "WR")]

set.seed(54321)
M = 2500;
k=5
##boostFit$cv.error[M]
boostFit <-nba.boost.shrink <-gbm(as.formula(formula),
                                data=explain_data,
                                distribution ="gaussian",
                                shrinkage = 0.05,
                                n.trees =M,
                                bag.fraction =1,
                                cv.folds =k,
                                n.minobsinnode= 3)


summary(boostFit, main="Effect of each attribute")
```
We used PTS, REB, AST, TOV, STL, BLK, Team, WR, AGE, FGM, FGPER, TPM, TPPER, FTM, FTPER, PF, PLUSMINUS, Position, Country, MIN, and Draft.Round against log(Salary), which is the same as what we used for Random Forest.

The result shows that Draft Round, Minutes played, and Age are the 3 major variates, with much higher influece over others.

```{r echo=FALSE}
formula = "log(Salary)~ Team + AGE + PTS + MIN+REB + AST + TOV + STL + 
          BLK + FGM + FGPER + TPM + TPPER + FTM + FTPER + PF + Position + 
          Country + PLUSMINUS + Draft.Round + WR"
explain_data = data[,c("Salary","PTS", "Team", "AGE","MIN", "PLUSMINUS", 
                      "Draft.Round", "REB", "AST", "TOV", "STL", "BLK", "FGM",
                      "FGPER", "TPM", "TPPER", "FTM", "FTPER", "PF", 
                      "Position", "Country", "WR")]

alphas <-c(0.005,0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,
           0.09,0.10,0.20,0.30,0.40,0.50,0.60,0.70,0.80,0.90,1)
n_alphas <-length(alphas)
cverror <-numeric(length =n_alphas)
Mvals <-numeric(length =n_alphas)
fit <-list(length =n_alphas)
for (i in 1:n_alphas) {
  set.seed(54321)
  fit[[i]] <- nba.boost.shrink <-gbm(as.formula(formula),
                                data=explain_data,
                                distribution ="gaussian",
                                shrinkage =alphas[i],
                                n.trees =M,
                                bag.fraction =1,
                                cv.folds =k,
                                n.minobsinnode= 3)
  cverror[i] <-min(fit[[i]]$cv.error)
  
  Mvals[i] <-which.min(fit[[i]]$cv.error)
  }
plot(alphas, cverror,type ="b",
     col =adjustcolor("firebrick",0.7),
     pch=19,lwd=2,main ="cross-validated error",
     xlab ="shrinkage",ylab="cv.error")
```
We see that the best learning rate for these 21 explanatory variates is at around 0.06, with cv error around 1.15.

```{r echo=FALSE}
alpha <- 0.06
k <- 5
# Shrinkage parameter values
Ms <- c(50, 100, 150, 200, 250, 300, 400, 500)
n_Ms <- length(Ms)
cverror <- numeric(length = n_Ms)
Mvals <- numeric(length = n_Ms)
fit <- list(length = n_Ms)
for (i in 1:n_Ms) {
  set.seed(54321)
  fit[[i]] <- nba.boost.shrink <- gbm(as.formula(formula),
                                  data=explain_data,
                                  distribution = "gaussian",
                                  shrinkage = alpha,
                                  n.trees = Ms[i],
                                  bag.fraction = 1,
                                  cv.folds = k
  )
  cverror[i] <- min(fit[[i]]$cv.error)
}
plot(Ms, cverror, type = "b",
     col = adjustcolor("firebrick", 0.7), pch=19, lwd=2,
     main = "cross-validated error", 
     xlab = "Number of trees", ylab="cv.error")
```
We would like to evaluate the best value for M, the total number of trees to fit, in order to find the minimum number of iterations needed to achieve the desired result having small cross validation error.
We see that the best value for M for more explanatory variables is at about 150, with cv error around 1.16. We can see that the return is very small when M is bigger than 150.

## Player self-evaluate and improvements

We want to do the same thing in boosting for what we did in random tree, allowing players to see what they need to improve on the most to get a higher salary. Again, we removed Team, MIN, WR, AGE, Draft.Round, and PLUSMINUS, and repeated the process.

```{r echo=FALSE}
formula = "log(Salary)~ PTS + REB + AST + TOV + STL + BLK + FGM + FGPER + TPM + 
                           TPPER + FTM + FTPER + PF + Position + Country"
explain_data = data[,c("Salary","PTS", "REB", "AST", "TOV", "STL", "BLK", "FGM",
                       "FGPER", "TPM", "TPPER", "FTM", "FTPER", "PF", "Position", "Country")]

set.seed(54321)
M = 2500;
k=5
##boostFit$cv.error[M]
boostFit <-nba.boost.shrink <-gbm(as.formula(formula),
                                data=explain_data,
                                distribution ="gaussian",
                                shrinkage = 0.05,
                                n.trees =M,
                                bag.fraction =1,
                                cv.folds =k,
                                n.minobsinnode= 3)


summary(boostFit, main="Effect of each attribute")
```
The result shows that PTS is the most important variable here, REB is the second most important, with TOV, FGM, FTPER, PF, and FGPER at 3rd to 7th, which are very close with each other.

```{r echo=FALSE}
alphas <-c(0.005,0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,
           0.09,0.10,0.20,0.30,0.40,0.50,0.60,0.70,0.80,0.90,1)
n_alphas <-length(alphas)
cverror <-numeric(length =n_alphas)
Mvals <-numeric(length =n_alphas)
fit <-list(length =n_alphas)
for (i in 1:n_alphas) {
  set.seed(54321)
  fit[[i]] <- nba.boost.shrink <-gbm(as.formula(formula),
                                data=explain_data,
                                distribution ="gaussian",
                                shrinkage =alphas[i],
                                n.trees =M,
                                bag.fraction =1,
                                cv.folds =k,
                                n.minobsinnode= 3)
  cverror[i] <-min(fit[[i]]$cv.error)
  
  Mvals[i] <-which.min(fit[[i]]$cv.error)
  }
plot(alphas, cverror,type ="b",
     col =adjustcolor("firebrick",0.7),
     pch=19,lwd=2,main ="cross-validated error",
     xlab ="shrinkage",ylab="cv.error")
```
We see that the best learning rate for these 15 explanatory variates is also at around 0.06 - 0.08, with cv error around 1.66.

```{r echo=FALSE}
alpha <- 0.06
k <- 5
# Shrinkage parameter values
Ms <- c(50, 100, 150, 200, 250, 300, 400, 500)
n_Ms <- length(Ms)
cverror <- numeric(length = n_Ms)
Mvals <- numeric(length = n_Ms)
fit <- list(length = n_Ms)
for (i in 1:n_Ms) {
  set.seed(54321)
  fit[[i]] <- nba.boost.shrink <- gbm(as.formula(formula),
                                  data=explain_data,
                                  distribution = "gaussian",
                                  shrinkage = alpha,
                                  n.trees = Ms[i],
                                  bag.fraction = 1,
                                  cv.folds = k
  )
  cverror[i] <- min(fit[[i]]$cv.error)
}
plot(Ms, cverror, type = "b", 
     col = adjustcolor("firebrick", 0.7), pch=19, lwd=2,
     main = "cross-validated error", xlab = "Number of trees", ylab="cv.error")
```


We see that the best value for M for more explanatory variables is also at about 200, with cv error around 1.66. We can see that the return is very small when M is bigger than 200.